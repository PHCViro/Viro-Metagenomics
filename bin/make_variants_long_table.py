#!/usr/bin/env python

import os
import sys
import glob
import errno
import logging
import argparse
import pandas as pd
import re
import numpy as np
from pandas import DataFrame

logger = logging.getLogger()

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)


def parser_args(args=None):
    Description = "Create long/wide tables containing variant information."
    Epilog = """Example usage: python make_variants_long_table.py --bcftools_query_dir ./bcftools_query/ --snpsift_dir ./snpsift/ --pangolin_dir ./pangolin/"""
    parser = argparse.ArgumentParser(description=Description, epilog=Epilog)
    parser.add_argument(
        "-bd",
        "--bcftools_query_dir",
        type=str,
        default="./bcftools_query",
        help="Directory containing output of BCFTools query for each sample (default: './bcftools_query').",
    )
    parser.add_argument(
        "-sd",
        "--snpsift_dir",
        type=str,
        default="./snpsift",
        help="Directory containing output of SnpSift for each sample (default: './snpsift').",
    )
    parser.add_argument(
        "-pd",
        "--pangolin_dir",
        type=str,
        default="./pangolin",
        help="Directory containing output of Pangolin for each sample (default: './pangolin').",
    )
    parser.add_argument(
        "-bs",
        "--bcftools_file_suffix",
        type=str,
        default=".bcftools_query.txt",
        help="Suffix to trim off BCFTools query file name to obtain sample name (default: '.bcftools_query.txt').",
    )
    parser.add_argument(
        "-ss",
        "--snpsift_file_suffix",
        type=str,
        default=".snpsift.txt",
        help="Suffix to trim off SnpSift file name to obtain sample name (default: '.snpsift.txt').",
    )
    parser.add_argument(
        "-ps",
        "--pangolin_file_suffix",
        type=str,
        default=".pangolin.csv",
        help="Suffix to trim off Pangolin file name to obtain sample name (default: '.pangolin.csv').",
    )
    parser.add_argument(
        "-of",
        "--output_file",
        type=str,
        default="variants_long_table.csv",
        help="Full path to output file (default: 'variants_long_table.csv').",
    )
    parser.add_argument(
        "-vc", "--variant_caller", type=str, default="ivar", help="Tool used to call the variants (default: 'ivar')."
    )
    parser.add_argument(
        "-re",
        "--resistance_mutations",
        type=str,
        default="resistance_muations.csv",
        help="File containing the resistance gene and mutations (default: 'resistance_muations.csv')."
    )
    parser.add_argument(
        "-cn",
        "--codonbases",
        type=str,
        default=None,
        help="File containing the codon bases (default: 'codonbases.csv')."
    )
    parser.add_argument(
        "-ofreg",
        "--variants_region_file",
        type=str,
        default="snpeff_variants_genomic_region.txt",
        help="File containing the annotations file."
    )
    parser.add_argument(
        "-ofimp",
        "--snpeff_variants_effect_impact_file",
        type=str,
        default="snpeff_variants_effect_impact.txt",
        help="File containing the annotations file."
    )
    parser.add_argument(
        "-oftyp",
        "--snpeff_variants_effect_type_file",
        type=str,
        default="snpeff_variants_effect_type.txt",
        help="File containing the annotations file."
    )
    parser.add_argument(
        "-offun",
        "--snpeff_variants_functional_class_file",
        type=str,
        default="snpeff_variants_functional_class.txt",
        help="File containing the annotations file."
    )
    parser.add_argument(
        "-ofqul",
        "--snpeff_quality_file",
        type=str,
        default="snpeff_quality.txt",
        help="File containing the annotations report file."
    )
    parser.add_argument(
        "-csv",
        "--snpeff_report_file",
        type=str,
        default=None,
        help="File containing the annotations report file."
    )
    parser.add_argument(
        "-bc",
        "--per_base_bed_file",
        type=str,
        default=None,
        help="Coverage file containing the per_base_bed_file generated by mosdepth."
    )
    return parser.parse_args(args)


def make_dir(path):
    if not len(path) == 0:
        try:
            os.makedirs(path)
        except OSError as exception:
            if exception.errno != errno.EEXIST:
                raise


def get_file_dict(file_dir, file_suffix):
    files = glob.glob(os.path.join(file_dir, f"*{file_suffix}"))
    samples = [os.path.basename(x).removesuffix(f"{file_suffix}") for x in files]

    return dict(zip(samples, files))


def three_letter_aa_to_one(hgvs_three):
    aa_dict = {
        "Ala": "A",
        "Arg": "R",
        "Asn": "N",
        "Asp": "D",
        "Cys": "C",
        "Gln": "Q",
        "Glu": "E",
        "Gly": "G",
        "His": "H",
        "Ile": "I",
        "Leu": "L",
        "Lys": "K",
        "Met": "M",
        "Phe": "F",
        "Pro": "P",
        "Pyl": "O",
        "Ser": "S",
        "Sec": "U",
        "Thr": "T",
        "Trp": "W",
        "Tyr": "Y",
        "Val": "V",
        "Asx": "B",
        "Glx": "Z",
        "Xaa": "X",
        "Xle": "J",
        "Ter": "*",
    }
    hgvs_one = hgvs_three
    for key in aa_dict:
        if key in hgvs_one:
            hgvs_one = hgvs_one.replace(str(key), str(aa_dict[key]))

    return hgvs_one


## Returns a pandas dataframe in the format:
#         CHROM   POS  REF  ALT FILTER   DP  REF_DP  ALT_DP    AF
# 0  MN908947.3   241    C    T   PASS  642     375     266  0.41
# 1  MN908947.3  1875    C    T   PASS   99      63      34  0.34
def ivar_bcftools_query_to_table(bcftools_query_file):
    table = pd.read_table(bcftools_query_file, header="infer")
    table = table.dropna(how="all", axis=1)
    old_colnames = list(table.columns)
    new_colnames = [x.split("]")[-1].split(":")[-1] for x in old_colnames]
    table.rename(columns=dict(zip(old_colnames, new_colnames)), inplace=True)

    if not table.empty:
        table[["ALT_DP", "DP"]] = table[["ALT_DP", "DP"]].apply(pd.to_numeric)
        table["AF"] = table["ALT_DP"] / table["DP"]
        table["AF"] = table["AF"].round(2)

    return table


## Returns a pandas dataframe in the format:
#         CHROM    POS REF ALT FILTER  DP REF_DP  ALT_DP    AF
# 0  MN908947.3    241   C   T      .  24      8      16  0.67
# 1  MN908947.3   3037   C   T      .  17      5      12  0.71
def bcftools_bcftools_query_to_table(bcftools_query_file):
    table = pd.read_table(bcftools_query_file, header="infer")
    table = table.dropna(how="all", axis=1)
    old_colnames = list(table.columns)
    new_colnames = [x.split("]")[-1].split(":")[-1] for x in old_colnames]
    table.rename(columns=dict(zip(old_colnames, new_colnames)), inplace=True)

    if not table.empty:
        table[["REF_DP", "ALT_DP"]] = table["AD"].str.split(",", expand=True)
        table[["ALT_DP", "DP"]] = table[["ALT_DP", "DP"]].apply(pd.to_numeric)
        table["AF"] = table["ALT_DP"] / table["DP"]
        table["AF"] = table["AF"].round(2)
        table.drop("AD", axis=1, inplace=True)

    return table


## Returns a pandas dataframe in the format:
#         CHROM   POS REF ALT FILTER  DP  REF_DP  ALT_DP    AF
# 0  MN908947.3   241   C   T   PASS  30       1      29  0.97
# 1  MN908947.3  1163   A   T   PASS  28       0      28  1.00
def nanopolish_bcftools_query_to_table(bcftools_query_file):
    table = pd.read_table(bcftools_query_file, header="infer")
    table = table.dropna(how="all", axis=1)
    old_colnames = list(table.columns)
    new_colnames = [x.split("]")[-1].split(":")[-1] for x in old_colnames]
    table.rename(columns=dict(zip(old_colnames, new_colnames)), inplace=True)

    ## Split out ref/alt depths from StrandSupport column
    if not table.empty:
        table_cp = table.copy()
        table_cp[["FORW_REF_DP", "REV_REF_DP", "FORW_ALT_DP", "REV_ALT_DP"]] = table_cp["StrandSupport"].str.split(
            ",", expand=True
        )
        table_cp[["FORW_REF_DP", "REV_REF_DP", "FORW_ALT_DP", "REV_ALT_DP"]] = table_cp[
            ["FORW_REF_DP", "REV_REF_DP", "FORW_ALT_DP", "REV_ALT_DP"]
        ].apply(pd.to_numeric)

        table["DP"] = table_cp[["FORW_REF_DP", "REV_REF_DP", "FORW_ALT_DP", "REV_ALT_DP"]].sum(axis=1)
        table["REF_DP"] = table_cp[["FORW_REF_DP", "REV_REF_DP"]].sum(axis=1)
        table["ALT_DP"] = table_cp[["FORW_ALT_DP", "REV_ALT_DP"]].sum(axis=1)
        table["AF"] = table["ALT_DP"] / table["DP"]
        table["AF"] = table["AF"].round(2)
        table.drop("StrandSupport", axis=1, inplace=True)

    return table


## Returns a pandas dataframe in the format:
#         CHROM    POS REF ALT FILTER  DP REF_DP  ALT_DP    AF
# 0  MN908947.3    241   C   T   PASS  21      0      21  1.00
# 1  MN908947.3   3037   C   T   PASS  28      0      25  0.89
def medaka_bcftools_query_to_table(bcftools_query_file):
    table = pd.read_table(bcftools_query_file, header="infer")
    table = table.dropna(how="all", axis=1)
    old_colnames = list(table.columns)
    new_colnames = [x.split("]")[-1].split(":")[-1] for x in old_colnames]
    table.rename(columns=dict(zip(old_colnames, new_colnames)), inplace=True)

    if not table.empty:
        table[["REF_DP", "ALT_DP"]] = table["AC"].str.split(",", expand=True)
        table[["ALT_DP", "DP"]] = table[["ALT_DP", "DP"]].apply(pd.to_numeric)
        table["AF"] = table["ALT_DP"] / table["DP"]
        table["AF"] = table["AF"].round(2)
        table.drop("AC", axis=1, inplace=True)

    return table


def get_pangolin_lineage(pangolin_file):
    table = pd.read_csv(pangolin_file, sep=",", header="infer")

    return table["lineage"][0]


def snpsift_to_table(snpsift_file):
    table = pd.read_table(snpsift_file, sep="\t", header="infer")
    table = table.loc[:, ~table.columns.str.contains("^Unnamed")]
    old_colnames = list(table.columns)
    new_colnames = [x.replace("ANN[*].", "") for x in old_colnames]
    table.rename(columns=dict(zip(old_colnames, new_colnames)), inplace=True)
    table_1 = table.loc[:, ["CHROM", "POS", "REF", "AF", "QUAL", "DP4", "ALT", "GENE", "EFFECT", "HGVS_C", "HGVS_P"]]
    table_2 = table.loc[:, ["CHROM", "POS", "REF", "AF", "QUAL", "DP4", "ALT", "GENE", "EFFECT", "HGVS_C", "HGVS_P"]]

    ## Split by comma and get first value in cols = ['ALT','GENE','EFFECT','HGVS_C','HGVS_P']
    for i in range(len(table_1)):
        for j in range(6, 11):
            table_1.iloc[i, j] = str(table_1.iloc[i, j]).split(",")[0]
            if j == 6:
                table_2.iloc[i, j] = str(table_2.iloc[i, j]).split(",")[0]
            else:
                table_2.iloc[i, j] = str(table_2.iloc[i, j]).split(",")[1]

    table = pd.concat([table_1, table_2], ignore_index=True, sort=False)
    
    table_firstpass = pd.DataFrame(columns=['CHR', 'POS', 'FWD_R', 'REV_R', 'FWD', 'REV'])
 
 
    for i in range(len(table)):
        percentage = table.iloc[i, 5].split(",")
        if percentage[0] != '.' and int(percentage[0]) + int(percentage[2]) != 0 and int(percentage[1]) + int(percentage[3]) != 0:
            new_row = {
                'CHR': table.iloc[i, 0],
                'POS': table.iloc[i, 1],
                # 'GENE': table.iloc[i, 7],
                'FWD_R': int(percentage[0]),
                'REV_R': int(percentage[1]),
                'FWD': int(percentage[2]),
                'REV': int(percentage[3])
            }
            table_firstpass = pd.concat([table_firstpass, pd.DataFrame([new_row])], ignore_index=True)

    table_firstpass = table_firstpass.drop_duplicates(keep='first')
    table_firstpass = table_firstpass.groupby(['CHR', 'POS', 'FWD_R', 'REV_R']).sum().reset_index()
    
    for i in range(len(table)):
            percentage = table.iloc[i, 5].split(",")
            if percentage[0] != '.' and int(percentage[0]) + int(percentage[2]) != 0 and int(percentage[1]) + int(percentage[3]) != 0:
                total_forward = table_firstpass[table_firstpass['POS'] == table.iloc[i, 1]]['FWD_R'].sum() + table_firstpass[table_firstpass['POS'] == table.iloc[i, 1]]['FWD'].sum()
                
                total_rev = table_firstpass[table_firstpass['POS'] == table.iloc[i, 1]]['REV_R'].sum() +table_firstpass[table_firstpass['POS'] == table.iloc[i, 1]]['REV'].sum()
                
                total = total_forward + total_rev
                percent_variant = (int(percentage[2]) + int(percentage[3])) / total
                forward = int(percentage[2]) / total_forward
                rev = int(percentage[3]) / total_rev
                table.iloc[i, 5] = str(percent_variant) + ',' + str(forward) + ',' + str(rev)                


    per_variant = []
    per_fwd = []
    per_rev = []
    for index, item in table["DP4"].items():
        per = item.split(",")
        if per[0] != '.':
            per_variant.append(per[0])
            per_fwd.append(per[1])
            per_rev.append(per[2])
        else:
            per_variant.append('.')
            per_fwd.append('.')
            per_rev.append('.')

    table.insert(loc=5, column='Percentage Variant', value=pd.Series(per_variant))
    table.insert(loc=12, column='Percentage Fwd Variant', value=pd.Series(per_fwd))
    table.insert(loc=13, column='Percentage Rev Variant', value=pd.Series(per_rev))
    cols = ['Percentage Variant', 'Percentage Fwd Variant', 'Percentage Rev Variant']
    table[cols] = table[cols].replace(['.'], np.nan)

    ## Amino acid substitution
    aa = []
    for index, item in table["HGVS_P"].items():
        hgvs_p = three_letter_aa_to_one(str(item))
        aa.append(hgvs_p)
    table.insert(loc=11, column='HGVS_P_1LETTER', value=pd.Series(aa))

    ## concat 1st two columns to make a unique name
    table.insert(loc=0, column='CHROM_POS_GENE',
                 value=table["CHROM"] + "_" + table["POS"].astype(str) + "_" + table["GENE"] + "_" + table["HGVS_C"])
    table.drop('DP4', axis=1, inplace=True)

    return table


def extract_data_from_ann_report(pattern, text):
    matches = re.findall(pattern, text, re.DOTALL)
    assert (matches)
    # Split the matched text into lines
    lines = matches[0].strip().split('\n')
    # Split each line into fields and create a list of dictionaries
    data = []
    for line in lines:
        fields = line.strip().split(',')
        if len(fields) == 3:
            entry = {
                'Type': fields[0].strip(),
                'Count': int(fields[1].strip()),
                'Percent': format(float(fields[2].replace('%', '').strip()) / 100, ".1%")
            }
            data.append(entry)
    df = pd.DataFrame(data)
    return df


def multibase_codon_detection(snpsift_df, codon_df):
    # Create an empty list to store the new dataframes
    result_df = pd.DataFrame(columns=['Codon', 'Gene', 'Bases'])
    # Iterate over each row in the hbv_codeonbases.csv
    for idx, row in codon_df.iterrows():
        # Check if at least two of the values for base1, base2, or base3 appear in the POS column of the snpsift.txt
        bases = [row['base1'], row['base2'], row['base3']]
        count = len(set(snpsift_df['POS']).intersection(bases))
        detected_bases = re.sub('\{|\}', '', str(set(snpsift_df['POS']).intersection(bases)))
        detected_bases = detected_bases.replace(',', ';')
        if count >= 2:
            # Create a new dataframe for each row that meets this condition
            new_row = pd.DataFrame({'Codon': [row['codon']], 'Gene': [row['Gene']], 'Bases': detected_bases})
            # append new row to the result_df
            result_df = pd.concat([result_df, new_row], ignore_index=True)

    return result_df


def add_coverage_for_each_position(merged_tables, per_base_bed_file):
    
    merged_tables['Coverage'] = 0
    # Read the per-base.bed.gz file. It has four columns: subtype, start_position, end_position, coverage.
    coverage_df = pd.read_csv(per_base_bed_file, sep="\t", header=None, names=["subtype", "start_position", "end_position", "coverage"])
    # For each row in the merged_tables, find the corresponding record in the per-base.bed.gz file based on the POS column
    for index, row in merged_tables.iterrows():
        pos = row["POS"]
        coverage_rows = coverage_df[(coverage_df["start_position"] <= pos) & (coverage_df["end_position"] > pos)]
        assert len(coverage_rows) == 1
        merged_tables.at[index, 'Coverage'] = coverage_rows.iloc[0]['coverage']

    return merged_tables        
    

def write_insertion_deletion_table(sample, merged_tables, genome):
    if genome =='CMV':
        insertion_deletion_table = merged_tables[merged_tables['AF'] == "."]
        # rename and drop columns
        insertion_deletion_table = insertion_deletion_table[['CHROM', 'GENE', 'POS', 'REF', 'ALT', 'AA', '%Var',
                                'QUAL', 'EFFECT', 'RES', 'HGVS_P', '%FwdVar', '%RevVar', 'AF', 'Coverage']]
        # apply filters
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table["EFFECT"] != "synonymous_variant"]
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table["EFFECT"] != "downstream_gene_variant"]
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table["EFFECT"] != "upstream_gene_variant"]
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table['EFFECT'] != "splice_region_variant"]
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table['QUAL'] > 9]
        insertion_deletion_table = insertion_deletion_table[insertion_deletion_table['Coverage'] > 8]
        insertion_deletion_table.reset_index(drop=True, inplace=True)
        insertion_deletion_table.to_csv(f"{sample}_insertion_deletion_table.csv", index=True, encoding="utf-8-sig")
    else:
        # drop all rows in insertion_deletion_table and save it to a file 
        # drop all rows in 
        insertion_deletion_table = merged_tables[0:0]
        insertion_deletion_table.to_csv(f"{sample}_insertion_deletion_table.csv", index=False)

def drop_low_coverage_records(merged_tables, per_base_bed_file):
    # Read the per-base.bed.gz file. It has four columns: subtype, start_position, end_position, coverage.
    # For each row in the merged_tables, find the corresponding record in the per-base.bed.gz file
    # if the coverage is less than 100, drop the row from the merged_table
    coverage_df = pd.read_csv(per_base_bed_file, sep="\t", header=None, names=["subtype", "start_position", "end_position", "coverage"])
    for index, row in merged_tables.iterrows():
        # Find the corresponding record in the per-base.bed.gz file
        pos = row["POS"]
        coverage_rows = coverage_df[(coverage_df["start_position"] <= pos) & (coverage_df["end_position"] > pos)]
        assert len(coverage_rows) == 1
        
        coverage = coverage_rows.iloc[0]
        if coverage["coverage"] < 100:
            merged_tables.drop(index, inplace=True)
    return merged_tables
    

def main(args=None):
    args = parser_args(args)

    # Create output directory if it doesn't exist
    out_dir = os.path.dirname(args.output_file)
    make_dir(out_dir)

    # Resistance Mutations
    resistance_mutations_file = args.resistance_mutations
    resistance_mutations = pd.read_csv(resistance_mutations_file, sep=",", header="infer")

    # Check correct variant caller has been provided
    variant_callers = ["ivar", "bcftools", "nanopolish", "medaka"]
    if args.variant_caller not in variant_callers:
        logger.error(
            f"Invalid option '--variant caller {args.variant_caller}'. Valid options: " + ", ".join(variant_callers)
        )
        sys.exit(1)
    
    if "HBV" in resistance_mutations_file:
        genome = "HBV"
    else:
        genome = "CMV"
    
    snpsift_files = get_file_dict(args.snpsift_dir, args.snpsift_file_suffix)
    samples = list(snpsift_files.keys())
    assert (len(samples) == 1)
    sample = samples[0]
    sample_table = snpsift_to_table(snpsift_files[sample])
    assert (not sample_table.empty)
    snpsift_tables: DataFrame = sample_table.copy(deep=True)
    
    merged_tables: DataFrame = sample_table.copy(deep=True)
    merged_tables = add_coverage_for_each_position(merged_tables, 
                                                   args.per_base_bed_file)
    
    merged_tables = process_samples_table(merged_tables= merged_tables,
                                          resistance_mutations=resistance_mutations)
    
    write_insertion_deletion_table(sample= sample, 
                                   merged_tables= merged_tables, 
                                   genome= genome)
    
    merged_tables = filter_samples_table_eff_prop_gene(merged_tables)
    merged_tables = drop_low_coverage_records(merged_tables, 
                                              args.per_base_bed_file)
    write_multi_nucleotide_table(sample, 
                                merged_tables,
                                resistance_mutations,
                                resistance_mutations_file)
    # order of the columns:
    merged_tables = merged_tables[['CHROM', 'GENE', 'POS', 'REF', 'ALT', 'AA', '%Var',
                                'QUAL', 'EFFECT', 'RES', 'HGVS_P', '%FwdVar', '%RevVar', 'Coverage']]
    
    if 'CMV' in resistance_mutations_file:
        merged_tables = merged_tables[
            ~merged_tables['EFFECT'].isin(['upstream_gene_variant', 'downstream_gene_variant'])]
        # reset the index based on the current order
        merged_tables = merged_tables.sort_values(by='RES', ascending=False)
        merged_tables.reset_index(drop=True, inplace=True)
        merged_tables.to_csv(f"{sample}_{args.output_file}", index=True, encoding="utf-8-sig")
        # create an empty file as a placeholder for the second varlist table in the repors. 
        open(f"{sample}_non_RT_domain_variants.csv", 'a').close()

    if 'HBV' in resistance_mutations_file:
        # breakdown the merged table into two tables: for GENE = 'RT_domain' and GENE != 'RT_domain'
        rt_domain_table = merged_tables[merged_tables['GENE'] == 'RT_domain']
        non_rt_domain_table = merged_tables[merged_tables['GENE'] != 'RT_domain']
        # write the two tables to separate csv files
        rt_domain_table.reset_index(drop=True, inplace=True)
        rt_domain_table.to_csv(f"{sample}_{args.output_file}", index=True, encoding="utf-8-sig")
        non_rt_domain_table.reset_index(drop=True, inplace=True)
        non_rt_domain_table.to_csv(f"{sample}_non_RT_domain_variants.csv", index=True, encoding="utf-8-sig")


    
    # Read the text file
    with open(args.snpeff_report_file, 'r') as file:
        text = file.read()

    # Define a regular expression pattern to match the block of text
    pattern = r"# Count by genomic region\n\nType , Count , Percent(.+?)\n\n"
    df = extract_data_from_ann_report(pattern, text)
    df.to_csv(f"{sample}_{args.variants_region_file}", sep='\t', index=False, encoding="utf-8-sig")

    # Varianst Effect by Impact file
    pattern = r"# Effects by impact\n\nType , Count , Percent(.+?)\n\n"
    df = extract_data_from_ann_report(pattern, text)
    df.to_csv(f"{sample}_{args.snpeff_variants_effect_impact_file}", sep='\t', index=False, encoding="utf-8-sig")

    # Variants by Effect Types file
    pattern = r"# Count by effects\n\nType , Count , Percent(.+?)\n\n"
    df = extract_data_from_ann_report(pattern, text)
    df.to_csv(f"{sample}_{args.snpeff_variants_effect_type_file}", sep='\t', index=False, encoding="utf-8-sig")

    # Variants by Functional Class file
    pattern = r"# Effects by functional class\n\nType , Count , Percent(.+?)\n\n"
    df = extract_data_from_ann_report(pattern, text)
    df.to_csv(f"{sample}_{args.snpeff_variants_functional_class_file}", sep='\t', index=False, encoding="utf-8-sig")

    # codon bases file
    codon_bases_file = args.codonbases
    # check if input argument is provided
    if codon_bases_file is not None:
        codon_df = pd.read_csv(codon_bases_file, sep=",", header="infer")
        results_df = multibase_codon_detection(snpsift_tables, codon_df)
        results_df.to_csv(f"{sample}_multichange_codons.csv", sep=',', index=False, encoding="utf-8-sig")

    # Quality
    pattern = r"# Quality \n\n(.+?)\n\n"
    matches = re.findall(pattern, text, re.DOTALL)
    assert (matches)
    lines = matches[0].strip().split('\n')
    data = {}
    for line in lines:
        parts = line.split(',')
        col_name = parts[0].strip()
        col_data = [int(value) for value in parts[1:]]
        data[col_name] = col_data
    df = pd.DataFrame(data)
    df.to_csv(f"{sample}_{args.snpeff_quality_file}", sep='\t', index=False, encoding="utf-8-sig")


def filter_samples_table_eff_prop_gene(in_table):
    # # Drop rows where QUAL less than 400.
    # merged_tables = merged_tables[merged_tables['QUAL'] >= 400]
    # remove rows with synonymous_variant
    in_table = in_table[in_table["EFFECT"] != "synonymous_variant"]
    # if the Effect column has splice_region_variant in its value, drop it
    in_table = in_table[in_table['EFFECT'] != 'splice_region_variant']
    # Drop rows where proportion is less than 1/5 or greater than 5
    in_table = in_table[((in_table['proportion'] <= 5) & (in_table['proportion'] >= 1 / 5))]
    # drop rows where GENE is one of preS1, preS2 and RNase_H_domain
    in_table = in_table[~in_table['GENE'].isin(['PreS1', 'PreS2', 'RNase_H_domain', 'X'])]

    return in_table


def process_samples_table(merged_tables, resistance_mutations):
    # add a column "look_up_value" for defining the lookup term
    merged_tables = merged_tables.copy()
    merged_tables.loc[:, 'look_up_value'] = merged_tables['HGVS_P_1LETTER'].str.replace('p.', '', regex=True)
    # for each row: if the value of HGVS_P_1LETTER is ".", and length of REF is 1 then set the value of
    # look_up_value to be the concatenation of REF, POS, ALT
    for idx, row in merged_tables.iterrows():
        if row['HGVS_P_1LETTER'] == '.' and len(row['REF']) == 1:
            merged_tables.loc[idx, 'look_up_value'] = f"{row['REF'].upper()}{row['POS']}{row['ALT'].upper()}"
        elif row['HGVS_P_1LETTER'] == '.' and len(row['REF']) > 1:
            merged_tables.loc[idx, 'look_up_value'] = ""
    merged_tables['RESISTANCE'] = create_drug_resistance_column(merged_tables, resistance_mutations)
    merged_tables = merged_tables.sort_values(by='RESISTANCE', ascending=False)
    merged_tables = merged_tables.reset_index(drop=True)

    merged_tables['proportion'] = (
            pd.to_numeric(merged_tables['Percentage Fwd Variant'].str.rstrip('%')) /
            pd.to_numeric(merged_tables['Percentage Rev Variant'].str.rstrip('%')))
    # rename and drop columns
    merged_tables = merged_tables.drop(columns=['HGVS_C'])
    merged_tables = merged_tables.drop(columns=['CHROM_POS_GENE'])
    merged_tables = merged_tables.rename(columns={'HGVS_P_1LETTER': 'AA'})
    merged_tables = merged_tables.rename(columns={'RESISTANCE': 'RES'})
    merged_tables = merged_tables.rename(columns={'Percentage Variant': '%Var'})
    merged_tables = merged_tables.rename(columns={'Percentage Fwd Variant': '%FwdVar'})
    merged_tables = merged_tables.rename(columns={'Percentage Rev Variant': '%RevVar'})
    
    return merged_tables

def create_drug_resistance_column(merged_tables, resistance_mutations):
    # add the extra column for the variants of interest based on the csv file.
    resistance = []
    for value in merged_tables['look_up_value']:
        found = False
        resistance_mutations_upper = resistance_mutations['mutation'].str.upper()
        for mut in resistance_mutations_upper:
            mutations = mut.split(',')
            if len(mutations) == 1 and value in mutations:
                resistance.append('Yes')
                found = True
                break
            elif len(mutations) > 1 and value in mutations:
                found = True
                if all(val in mutations for val in merged_tables['look_up_value']):
                    resistance.append('Yes')
                    break
                else:
                    resistance.append('Partial')
                    break
        if not found:
            resistance.append('No')
    return resistance


def write_multi_nucleotide_table(sample, merged_tables, resistance_mutations, genome):
    # for the records that have resistance== yes, add the columns from the resistance_mutations_file
    multi_nucleotide_table = merged_tables[merged_tables['RES'] == 'Yes']
    # join the multiNucleotideTable with the resistance_mutations on mutation from resistance_mutations and
    # look_up_value from multiNucleotideTable
    multi_nucleotide_table = pd.merge(multi_nucleotide_table, 
                                      resistance_mutations, 
                                      left_on='look_up_value',
                                      right_on='mutation',
                                      how='left')
    # drop mutations column and rename the column look_up_value to mutation
    multi_nucleotide_table = multi_nucleotide_table.drop(columns=['mutation'])
    multi_nucleotide_table = multi_nucleotide_table.rename(columns={'look_up_value': 'Mutation'})
    # List of desired columns
    desired_columns = ['GENE', 'AA', '%Var', 'RES',
                       'Cidofovir', 'Foscarnet', 'Ganciclovir', 'Maribavir', 'Letermovir',
                       'Lamivudine', 'Adefovir', 'Entecavir', 'Tenofovir', 'Reference']

    # Filter the list to include only columns that are present in the DataFrame
    available_columns = [col for col in desired_columns if col in multi_nucleotide_table.columns]

    # Select the available columns from the DataFrame
    multi_nucleotide_table = multi_nucleotide_table[available_columns]
    
    # if it is an HBV sample, drop rows that do not have mutation in RT_domain
    multi_nucleotide_table = multi_nucleotide_table[(multi_nucleotide_table['GENE'] != 'core') & (multi_nucleotide_table['GENE'] != 'precore')]
    
    multi_nucleotide_table.to_csv(f"{sample}_drug_resistance_mutations.csv", index=True, encoding="utf-8-sig")


if __name__ == "__main__":
    
    sys.exit(main())
